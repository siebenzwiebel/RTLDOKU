### 3.4.1 Einlesen von WAV-Dateien — read_wav()

Wav Dateien werden mithilfe der Library libsndfile und der dazugehörigen Funktion sf_open() eingelesen. Liegen Dateien nicht im WAV-Format vor, werden sie vorab mittels der LAME-Library ins WAV-Format konvertiert.

WAV-Dateien mit mehr als einem Kanal werden außerdem innerhalb der Funktion read_wav() in Mono-Dateien konvertiert um die folgende Analyse zu vereinfachen. Da Samples für unterschiedliche Kanäle im WAV-Format immer direkt aufeinanderfolgen lässt sich die Trennung durch eine simple Schleife realisieren, die die Signalwerte der Kanäle pro Sample mittelt.

### 3.4.2 Fourier Transformation

Die Fourier-Transformation (oder Spektralfunktion) ist eine im Jahr 1822 von Jean Baptiste Joseph Fourier aufgestellte Funktion, die kontinuierliche, aperiodische Signale in ein kontinuierliches Spektrum zerlegt [Wikipedia: Fourier-Transformation](https://de.wikipedia.org/wiki/Fourier-Transformation).

![enter image description here](https://lh3.googleusercontent.com/tl3J7YrSPNOmJWLXNnWxAjfCobZ1V1YARvx9W-ruv8z0n61V273yEPoqt4JRRdDMv1TqUGkyd9k "FT")

Die Fourier-Transformierte ist somit die Summe der Integrale aller Beträge(x(t)), multipliziert mit ihrer Phase. Hier reicht jedoch noch keine Summe, da jedes Signal idealerweise unendlich ist.

Anwendung findet sie bei uns, indem sie das Signal (hier unsere Songs) in ihre verschiedenen Frequenzen, aus denen sich das Signal zusammensetzt, aufteilt. Dadurch können wir bestimmte Frequenzen bestimmten Bereichen zuordnen und das ansteuern der Lampen erleichtern. So können wir Bass, Mitten und Höhen auf einzelne Lampen legen, ohne dass die anderen Frequenzen Einfluss auf die Lampe haben. 

Da unsere Signale endliche Signalwerte besitzen, benutzen wir die Diskrete Fourier-Transformation(DFT), um die vorkommenden Frequenzen und die Amplituden in unseren Liedern zu berechnen.
![enter image description here](https://lh3.googleusercontent.com/0fswfdH_ApnJlLq-Zwr0GUdc1RKn5RrvP1FQQnGCRjY8b9Wm8mq2XMf4501Y3baMiuz8My_uZ_o "DFT")

Aus dem Integral kann nun eine Summe gebildet werden, da uns jetzt die Anzahl der Signalwerte bekannt ist, die wir verarbeiten [Wikipedia: Diskrete Fourier-Transformation](https://de.wikipedia.org/wiki/Diskrete_Fourier-Transformation).

![enter image description here](https://lh3.googleusercontent.com/44GHgAqOqokZlg70nfRQVVAWgDQArJfs2EO7Iiu4IvrJz2OM7cA1qz6r6FfIRTeqJP3oVOVQjFA "DFT2")

Zur Vereinfachung kann bei einer endlichen Anzahl von Elementen die Phase e^-2![ \pi ](https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a)i*jk/N durch Z ersetzt werden. Die Potenz von Z steigt mit dem Index von a, damit wir die Summe weglassen können, die die Phase der Funktionswerte beeinflusst. 

Realisiert wird die FFT durch den Algorithmus der library FFTW3.

### FFTW3

FFTW3 ist eine in ANSI C geschriebene Library, welche auf vielen Rechnerarchitekturen funktioniert und dabei vergleichsweise schnell arbeitet. Während die meisten Programme einen Aufwand von O(n^2) für die Berechnung benötigen, braucht FFTW3 nur O(n log n). Da die Laufzeit von vielen äußeren Faktoren abhängig ist (Hardware, parallel laufende Programme und Berechnungen, etc.), bestimmen wir die Effizienz stattdessen mithilfe der O-Notation. Die O-Notation gibt Auskunft über die nötigen Arbeitsschritte, die ein Algorithmus bei einer Eingabelänge n benötigt. Das ermöglicht eine Einteilung der Programme in verschiedene Klassen (z.b. polynominal, konstant, exponentiell). 
So benötigt FFTW3 `54.769.906` Arbeitsschritte, während ein quadratische Algorithmus mit `63.011.844.000.000`Arbeitsschritten deutlich ineffizienter arbeitet. 


Die Library FFTW3 teilt sich auf in 2 wesentliche Bestandteile: Den Planner und den Executor. 

![](https://lh3.googleusercontent.com/hfeBkztiLOsO-7Rbkqjt_o1jW6fQviq6Pd9ANNtBLdqAmp-zk1w1t7wFQE4ml97sXD5mAeboWTw "Planner")

Der FFTW Planner vergleicht alle möglichen Wege der Durchführung und sucht daraus den am schnellsten ausführbaren Weg. Zuerst müssen vom User Strukturen wie Arrays und Speicherplatz festgelegt werden, mit denen die Datei bearbeitet werden soll. Das "Problem" soll also spezifiziert werden. Anschließend teilt der Planner diese Probleme rekursiv in weitere Teilprobleme auf, bis sie sich nicht mehr zerlegen lassen. Wenn diese Probleme einfach genug sind, wird vom Planner ein Code aufgerufen, der dieses Problem sofort lösen kann (so gennnante `Codelets `). Der Aufruf dieser Funktionen wird in einem Plan festgehalten, der von dem Planner als Datenstruktur an den Executor weitergeleitet wird.

![](https://lh3.googleusercontent.com/CdZvwgtKbJEqNm4eMMP4PWvOiicBXckWwnpcsTlu8mwlSbEvFQTnrUOkR1V3605U7S6xP5qOERlC "EXEC")

Hier wird unsere Datei verarbeitet. Der vorher erstellte Plan ermöglicht die schnellst mögliche Verarbeitung unserer Lieder. Dabei wird auf jedes kleinstmögliche Problem das vorher vom Planner herausgesuchte Codelet angewendet. Der Algorithmus arbeitet schneller als die meisten Algorithmen zur Arbeit mit der Fourier-Transformationen, dennoch benötigt er einmalig etwas mehr Zeit, während ein Plan zur Verarbeitung erstellt wird [http://www.fftw.org/fftw-paper-ieee.pdf](http://www.fftw.org/fftw-paper-ieee.pdf). 

![](https://lh3.googleusercontent.com/0QqrBgPzaF8IkGFqrLGUf_74PseY-xqcEqixJLDcT7ExEwEDQ03htBXH6LJ4P5RuKoYqejOYxeWr "Gesamt")

### STFT (Short Time Fourier Transformation)

Die Short Time Fourier Transformation, zu deutsch Kurzzeit-Fourier-Transformation oder auch kurz STFT, besteht aus einer Reihe von zeitlich lokalisierten Fourier-Transformationen gefensterter Datensegmente.

Im Gegensatz zur Fourier-Transformation, im Folgenden FT genannt werden bei der STFT die Daten nicht über die gesamte Signallänge gemittelt (Spektrum), sondern üblicherweise in einem 3D-Datensatz dargestellt, um spektrale Änderungen eines Signals über die Zeit visuell nachvollziehbar zu machen. Die drei dargestellten Komponenten sind die Zeit (Abszisse), Frequenz (Ordinate) und die Magnitude (Färbung). Jeder FT (FFT) wird dabei der Zeitpunkt zugeordnet, der der Mitte des Fensters entspricht.

![enter image description here](https://www.researchgate.net/profile/Antoni_Torras-Rosell/publication/42252354/figure/fig1/AS:669994794889220@1536750615201/Spectrogram-of-a-linear-sweep-synthesized-in-the-time-domain-The-signal-is-1-s-long.png)
Abbildung: Spektrogramm eines linearen Sweeps ([Quelle](https://www.researchgate.net/profile/Antoni_Torras-Rosell/publication/42252354/figure/fig1/AS:669994794889220@1536750615201/Spectrogram-of-a-linear-sweep-synthesized-in-the-time-domain-The-signal-is-1-s-long.png))

In unserem Fall wird, wie schon im Abschnitt zur FT erläutert, nicht die zeitkontinuierliche Variante der Transformation implementiert, sondern die zeitdiskrete.

Das STFT-Paar ist somit gegeben durch: 

![](https://lh3.googleusercontent.com/kltswcqh3GJaSLYAO0pJDNfgD_AuqPKu0iKW9UHC6kBjY4eDbxItdhCiYvgUM65-GmaaWsxXAhD6 "STFT")

wobei x[k] ein endliches Signal und g[k] eine Fensterfunktion mit der Breite L ist.

Die Fensterung des Signals ist nötig, um die Auswirkungen, des Leck-Effektes zu verringern. Wir benutzen dazu ein  Hanning-Fenster (Siehe Flussdiagramm zur STFT).

Ein weitere Problematik ergibt sich aus der "Kupfmüllerschen Unbestimmtheitsrelation" auch "Unschärferelation der Nachrichtentechnik" genannt (analog zur "Heisenbergschen Unschärferelation"), die besagt, dass die Zeitdauer Δt und Bandbreite Δf eines Signals nicht gleichzeitig beliebig klein werden können, da das Produkt aus Δt und Δf stets größer/gleich k ist. k nimmt dabei je nach Definition von Bandbreite und Zeitdauer den Wert 1 oder 0,5 an.

Veranschaulichen kann man diese Problematik an folgendem Beispiel:

Mit einer FFT von 4 WAV-Samples (44,1kHz Samplerate) analysiert man zwar einen sehr genauen Zeitraum von ca. 90,7 Mikrosekunden, erhält allerdings nur 4 DFT-Koeffizienten. Davon sind aufgrund der Symmetrieeigenschaften der FFT nur zwei relevant, ergo wird das gesamte Spektrum bis 22,05kHz nur von zwei DFT-Koeffizienten abgedeckt.

Für eine bessere Frequenzauflösung sind also mehr Sample-Werte nötig. allerdings wird dabei die zeitliche Auflösung geringer.

Hier kann man allerdings etwas nachhelfen, indem man die gefensterten Sample-Blöcke überlappt. D.h. bei einer Überlappung von 50% würden die folgenden Signalblöcke transformiert:

FT1: Sample 1-1024 
FT2: Sample 513-1536
FT3: Sample 1025-2048
u.s.w.

Dadurch erreicht man eine höhere zeitliche Auflösung ohne Verringerung der Frequenzauflösung.

Unsere Umsetzung der STFT basiert größtenteils auf folgendem Code-Beispiel: [Short Time Fourier Transform with FFTW3](http://ofdsp.blogspot.com/2011/08/short-time-fourier-transform-with-fftw3.html)

Das folgende Flussdiagramm verdeutlich den Datenfluss innerhalb der von uns umgesetzten Funktion `stft()`.

![](https://lh3.googleusercontent.com/VP7c1B9m1JjQFTJ336aBf7sE-AS1SGJaAXyugjrUw37aCwLW8YELKHTyiZBBR12ffqjFdvdE1SNV "stft_flowchart")


### Bandpass — peaks_per_band()

Die erste tatsächliche Nutzung der aus der STFT gewonnen Daten im Projekt fand in der Funktion peaks_per_band() statt, die auch im finalen Projekt noch genutzt wird.

Sie fungiert als Bandpass für die Ergebnisse der STFT und gibt in Schritten von 25ms, also mit 40Hz die Betragsquadrate der Koeffizienten für ein frei gewähltes Frequenzband aus, sodass bestimmte Lampen z.B. nur mit den Werten aus dem Bass-Bereich bedient werden können.

## 3.4.3 Beat Detection

Die Beat Detection verfolgt dem natürlichen Phänomen den Beat aus einem Musikstück zu erkennen. In der Musikanalyse ist die Beat Detection weiterhin ein großes Problem. Eine Abwägung zwischen Geschwindigkeit und Genauigkeit der Beat Detection muss immer gefällt werden. Es gibt viele Algorithmen mit unterschiedlichem Ansatz zur Problemlösung der Beat Detection. Wir haben uns in der Analyse für den Comb-Filter-Algorithmus zur Beat Detection geeinigt.

Der Comb-Filter-Algorithmus beruht auf der Korrelation von zwei Signalen.

![enter image description here](https://lh3.googleusercontent.com/paoHJH51FaIlxhx3BXA77yWWfDeFD8JYa55n3skQtEG-G2cTN5wnrR3geCZkJbPVV4ef4PVGARxR)

1) Korrelation

Die Korrelation vergleicht zwei Signale auf ihre maximale Ähnlichkeit. Dieser Wert ist im Wertebereich [-1;1], wobei bei 1 und -1 eine maximale Korrelation vorliegt und bei 0 keine Korrelation vorliegt. Der Faktor Alpha ist im Comb-Filter-Algorithmus immer 1 um die Korrelationsenergie zwischen beiden Signalen zu berechnen. Der Faktor Beta dient zur Lösung der Zeitachsen-Translation. Zwei Signale können dadurch,unabhänging von der Verschiebung auf der Zeitachse,miteinander verglichen und somit die Korrelation berechnet werden.

![enter image description here](https://lh3.googleusercontent.com/dvfSjtglS6WSPANm-zsMpwQGe482zKumpDrweqVjjzz166Y7QfttfUIxMqeO0af31vpgr6fZZLdd)

2) Korrelation Comb-Filter-Algorithmus

Das Signal x[k] ist ein Liedausschnitt, welches alle signifikanten Information zum Beat des Liedes enthält. Die Auswahl des Ausschnitts beruht darauf, dass der maximale Samplewert im Lied gesucht wird und eine Zeitspanne von 3.5 Sekunden vor und 3.5 Sekunden nach dem gefundenen Samplewert als Liedausschnitt ausgewählt wird.

Das Signal y[k] ist eine Impulsfolge einer bestimmten Frequenz, welche in Abhängigkeit von der getesteten beats per minute berechnet wird.

![enter image description here](https://lh3.googleusercontent.com/xo95-oFtuCoN1O6G5KkMVUNqMcOxAH74Ybz4wkGYOwGb21AwXnzR4GtB3FJ1YANYTAbNJWLrOJTb)

3) Formel BPM fs=Abtastfrequenz Ti=Samples zwischen einzelnen Beats

Durch die Verschiebung des Signals y[k] durch den Faktor Beta ist der Aufwand zur Berechnung der Korrelation enorm hoch. Abhilfe für dieses Problem ist die Fourier-Transformation, wodurch die Verschiebung des Signals y[k] durch den Faktor Beta überflüssig wird. Die Fourierkoeffizienten sind unabhängig von der Verschiebung und somit immer gleich. Die Korrelationsenergie wird direkt im Frequenzbereich berechnet und senkt somit den Rechenaufwand.

![enter image description here](https://lh3.googleusercontent.com/20xqsbKXbTB1n8V54CggqvsauXVAyu0D04qMyp4GgeovgDIxC3dKXTxM32MrhgeWpq-GMoxqmQnh)

4) Korrelation Frequenzbereich

Die Korrelationsenergie gibt Aufschluss darüber wie ähnlich der Liedausschnitt und die getestete Impulsfolge ist. Umso höher der Wert der Korrelationsenergie ist, umso ähnlicher sind sich die beiden Signal. Die getesteten beats per minute mit der maximalen Korrelationsenergie sind somit die gefundenen beats per minute des Comb-Filter-Algorithmus.

### Funktion get_bpm()

Die Funktion get_bpm() führt den Comb-Filter-Algorithmus aus und ermittelt somit die gesuchten beats per minute. Die Funktion benötigt den ermittelten Liedausschnitt wav_values_mono_snippet[], welcher als Vektor übergeben wird. Der Vektor wav_values_mono_snippet[] wird zunächst Fourieranalysiert, die Ergebnisse des Realteils werden im Vektor resultRE[] gespeichert und die Ergebnisse des Imaginärteils im Vektor resultIM[] gespeichert.

![enter image description here](https://lh3.googleusercontent.com/6B7eN_U2Xtm0IZfc_1KxkX4kikOkQCWWy9-09qW_yTEmcSbnzuuCvMnNlovzgpMnU3oMBsOHS6Dr)

5) FFT der Funktion get_bpm()

Mit Hilfe von 3) wird eine Impulsfolge in Abhängigkeit von den getesteten beats per minute erzeugt. Wobei zu beachten ist, dass in jeder Impulsfolge gleichviele Impulse vorhanden sein müssen. Ohne diese Einschränkung werden verfälschte Ergebnisse erzeugt, da die Korrelationsenergie proportional zur Anzahl der Impulse steigt. Die Operation fmod(k,Ti) berechnet den Rest einer Division und kann mit Gleitkommazahlen Rechnungen ausführen. Der double Wert Ti ist der Abstand zwischen den einzelnen Impulsen in Abhängigkeit der beats per minute. Der int Wert k ist der momentane Samplewert der erzeugten Impulsfolge. Erzeugt die Operation fmod(k,Ti) einen Wert kleiner als 1, steht fest das an der Stelle k ein Impuls ist. Ist der Wert größer als 1, dann ist an dieser Stelle kein Impuls. Die erzeugten Ergebnisse werden in einem Vektor impulse[] gespeichert.

![enter image description here](https://lh3.googleusercontent.com/_P8oSjNp-T5425_ynxuE2oQSxkgRIkq8YYqVOl5ZlC8OWJIHE8RZm97kDStTYlQVpNf2YHVdmQgo)

6)Erzeugung der Impulsfolge

Die erzeugte Impulsfolge impulse[] wird an die Funktion fft_impulse() übergeben und führt eine Fourier-Transformation durch, wobei die Ergebnisse des Realteils in impulseFFT.real[] und die Ergebnisse des Imaginärteils in impulseFFT.imag[] gespeichert sind.

Um den natürlichen Gehörgang nachzubilden, wird der Frequenzbereich logarithmisch in 16 Abschnitte unterteilt. Dadurch sind im ersten Band die meisten Fourierkoeffizienten, welche tiefe Frequenzen darstellen, und somit die stärkste Korrelationsenergie. Für die Analyse wird ausschließlich das erste Band benutzt, da dort das aussagekräftigste Ergebnis enthalten ist. Es besteht die Möglichkeit andere Frequenzbänder, durch Änderung des Code, in Betracht zu ziehen.

Die Korrelationsenergie der zu testenden beats per minute und dem Liedausschnitt wird wie in Formel 4 berechnet und ist im Code wie folgt realisiert.

![enter image description here](https://lh3.googleusercontent.com/SGx2sSxafsZJfJZrU_w0gNy-KFoMQ-bMzAJ7na8aVMDWWZIMhVNoCfHFaMvghFsR4IsmKzc14QqO)

7) Berechnung der Korrelationsenergie

Das Ergebnis ernergyValue wird in einem Vektor EBPMcs[] gespeichert.

Durch die for-Schleife:

![enter image description here](https://lh3.googleusercontent.com/NxkTUnJX0lYuv--hv7wSiqdGXfV2ww7dbswLlT9lyw3xVaBgdZWEsmaZE5NYFyozLCE_0yjNOKRx)

8) for-Schleife BPM

Wird automatisch zur nächsten zu testenden beats per minute gesprungen und die Korrelationsenergie berechnet. Sobald die Schleife durchlaufen ist, ist die Korrelationsenergie aller zu testenden beats per minute im Vektor EBPMcs[] gespeichert. In einer weiteren for-Schleife wird die maximale Korrelationsenergie des ersten Bandes im Vektor EBPMcs[] berechnet und die dazugehörige beats per minute als finalBPM gespeichert.

![maxKorrelationsEnergieSchleife.PNG](C:\Users\Timo\Desktop\BeatDetection\DokumentationBilder\maxKorrelationsEnergieSchleife.PNG)

9) for-Schleife zur Suche der maximalen Korrelationsenergie

![enter image description here](https://lh3.googleusercontent.com/R3PXeeLgU9Tsj9-bFpsWECz1RUrwbyAQKWPb4L8AYQ8TzR-VebK8xDYdus4i39LaVstNpDM7dd28)

10) Ablaufdiagramm der Funktion get_bpm()

### Funktion fft_impulse()

Die Funktion fft_impulse() wird in der Funktion get_bpm() aufgerufen und bekommt als Übergabeparameter den Vektor impulse[], der die erzeugte Impulsfolge enthält. Der Vektor impulse[] wird einer Fourieranalyse unterzogen und die Ergebnisse werden in einem struct result{} gespeichert. Der struct result{} enthält 2 Vektoren, real[] und imaginär[]. In den Vektoren werden jeweils Realteil und Imaginärteil der Fouriertransformation gespeichert und zurück an die Funktion get_bpm() übergeben.

![enter image description here](https://lh3.googleusercontent.com/8lDEpd-h38SratQb2mdqNn-qGSmOlAiXh9MjUvvoj6Vcx4ebWlINQOWcMGeU6J8Zbt_1_jd5BHxv)

11) for-Schleife FFT Impulsfolge

### Funktion get_first_beat()

Die Funktion get_first_beat() ist aus einem Algorithmus zur Beat Detection entstanden. Der Algorithmus hat für die gesuchten beats per minute keine guten Ergebnisse geliefert und ist Abhängig von einem Faktor, der für jedes Lied unterschiedlich ist. Zur Detektion von einzelnen Beats eignet sich der Algorithmus und liefert gute Ergebnisse.

Der Algorithmus berechnet die short time fourier transformation, für eine Blockgröße von 1024, über das ganze Lied. Eine Überlappung ist nicht erforderlich, da die Ergebnisse zufriedenstellend sind und somit der Rechenaufwand verringert werden kann. Aus dem Realteil und dem Imaginärteil des Ergebnisses der Fourieranalyse, wird der Betrag gebildet und im Vektor first_beat_result[] gespeichert.

![enter image description here](https://lh3.googleusercontent.com/pHdAYANM5S5MKMBv3j-OxMN27l36rngyeg1B5-T7I8H0m8Vkq9w02BH9VRJi56nx369BlPIqDBbB)

12)for-Schleife STFT

Die Ergebnisse der einzelnen Blöcke werden logarithmisch in verschieden Frequenzbänder aufgeteilt. Genauso wie in der Funktion get_bpm() wird nur das erste Band benutzt.


Der Vektor energySubband[] enthält die Ergebnisse der einzelnen Blöcke für das erste Band. Zur Detektierung des ersten Beats wird aus dem Vektor energySubband[] der maximale Wert ermittelt und in der Variable maxResult gespeichert.

![enter image description here](https://lh3.googleusercontent.com/RgyOb-2wSobITTfCF9EkzRRepckINPxTULBxsBEG1qbR_E9IGumIcSAlzU3R5IdjGJWFj3Fri3oV)

13) Ergebnisse der Blöcke werden gespeichert und das Maximum ermittelt

Die einzelnen Blöcke entsprechen ca. einer Zeit von 23ms, somit entsprechen 43 Blöcke ca. 1 Sekunde des Liedes. Um zu überprüfen ob ein Beat im momentanen Block vorliegt, wird der Energiedurchschnitt des Bandes für die nächsten 43 Blöcke gebildet und geguckt ob der momentane Block die 1.5-fache Energie des Durschnitts der 43 Blöcke übersteigt. Zusätzlich wird überprüft ob die Energie des momentanen Blockes mindestens halb so Groß wie die maximal vorkommende Energie des Bandes ist. Durch diese zusätzliche Überprüfung wird sichergestellt, dass kein Rauschen als Beat detektiert wird. Wenn diese 2 Kriterien zutreffen ist ein Beat gefunden worden. Dieser wird in den Vektor time[] gespeichert und die suche nach weiteren Beats wird abgeprochen. Als Rückgabeparameter hat die Funktion get_first_beat() die Variable timeTemp, welche den Samplezeitpunkt des ersten Beats enthält.

![enter image description here](https://lh3.googleusercontent.com/RdunJL7-3VaNQotO4dqGv2ONiZnsvAljulviVODy4BBENXz6f-fzwf8Va3kFGll5G7YHDh-hZROe)

14) Detektierung des ersten Beats

Durch die Faktoren zur Suche eines Beats, wird deutlich wieso der Algorithmus keine zufriedenstellenden Ergebnisse zur Ermittlung der beats per minute liefert. Die Faktoren sind Liedabhängig und können nicht allgemeingültig für alle Lieder gewählt werden.Zur Detektierung einzelner Beats eignet sich der Algorithmus gut und kann beinahe für alle Lieder benutzt werden. Bei einigen Liedern im Genre Ambient und Filmmusik werden falsche Beats detektiert, wobei hier die Frage ist ob eine Beat Detection überhaupt sinnvoll ist. Geringe Ausschläge in einer ruhige Passage werden dort als Beat erkannt und gespeichert. Um diesem Problem entgegenzuwirken, wäre es möglich die genaue Frequenz der Beats zu analysieren und nur in diesem Frequenzbereich nach Beats zu suchen.

### Funktion get_all_beats()

Die Funktion get_all_beats() ermittelt aus den Variablen finalBPM und timeTemp, welche die beats per minute und den Samplezeitpunkt des ersten Beats enthalten, alle anderen vorkommenden Samplezeitpunkte der Beats im Lied. Hierzu wird aus den ermittelten beats per minute der Abstand der einzelnen Beats ermittelt. In Abhängigkeit vom Samplezeitpunkt des ersten Beats und dem errechneten Abstand der einzelnen Beats wird der Vektor time[] mit den Samplezeitpunkten aller anderen Beats gefüllt und von der Funktion get_all_beats() zurückgegeben.

## 3.4.4 Segmentierung

Eines der größten Probleme der Analyse bestand in der musikalischen Segmentierung von Liedern.

In der Literatur gibt es verschiedenste Ansätze zur allgemeinen Segmentierung von Musik, z.B. [durch Selbstähnlichkeitsmatrizen basierend auf MFC-Koeffizienten](http://www.musanim.com/wavalign/foote.pdf).

Durch die hardwareseitige Leistungsbegrenzung und Projektvorgabe der Analysedauer musste allerdings ein einfacherer bzw. weniger aufwändiger Weg gewählt werden. (Durch das Erscheinen des Raspberry Pi 4 ist die Umsetzung einer solche Analyse-Methode für zukünftige Erweiterungen in Betracht zu ziehen)

### Annäherung einer Intensitätsfunktion

Im ersten Schritt wurde versucht die Intensitätskurve eines Musikstücks mithilfe des Energiedichtespektrums (PSD, Power Spectral Density) anzunähern. Dazu wurde die PSD für alle Bins der STFT gebildet und geplottet.

Hier wurde schnell klar, dass trotz der Wahl unterschiedlichster Blockgrößen, die daraus resultierende Kurve nicht "glatt" genug war, um repräsentativ für ein Musikstück zu sein. Um eine Glättung der Kurve zu erzielen, wurde ein nicht-kausaler gleitender Mittelwertbilder mit binomial verteilter Gewichtung (Siehe 3.3.6 Gleitender Mittelwertbilder) geschrieben und auf die Intensitätswerte, bzw. PSD-Werte angewendet.

Diese Kurve wurde mit einer manuellen Segmentierung der Musikstücke (Intro, Strophe, Refrain, etc.) verglichen und man konnte erkennen, dass sich im Bereich der Segmentsprünge (also von z.B. Strophe auf Refrain) Wendestellen der Kurve befanden. Mithilfe der Funktion get_turning_points(), wurden diese Wendestellen mathematisch bestimmt und wieder mit den manuell bestimmten Zeitpunkten der Segmentänderung verglichen. Hier fiel auf, dass einerseits mehr Wendepunkte gefunden wurden als gewünscht und—noch viel wichtiger—die gefundenen Zeitpunkte meist um bis zu 0,5s von den manuell bestimmten Werten abwichen.

An diesem Punkt hätte man die gefunden Wendestellen nur noch auf den nächsten Beat aus der Beat Detection legen müssen und hätte so einen genauen Zeitpunkt der Änderung erhalten. Bei Liedern, die teilweise Tempi von über 120 Beats per Minute aufweisen, ist die Abweichung allerdings zu groß um den richtigen Beat zu bestimmen.

Abhilfe für dieses Problem wurde durch einen neuen Ansatz geschaffen bei dem man die Intensitäten ausgehend von der Beat Detection bestimmt.

### Zweiter Ansatz – get_intensity_average_for_next_segment()

Basierend auf dem Tempo des Songs, wurde eine STFT ausgeführt, deren block_size und hop_size so berechnet wurden, dass die daraus resultierenden Zeitpunkte der Intensitäten immer genau dem ersten Schlag einer bestimmten Anzahl von Takten entsprechen. Eine Bestimmung des Metrums (3/4, 4/4, 7/8) wurde im Rahmen der Beat Detection nicht umgesetzt, sodass hier immer von einem 4/4-Takt ausgegangen wird.

Da man sich nicht darauf verlassen kann, dass Song-Segmente immer auf einem Vielfachen von vier Takten basieren, auch wenn dies z.B. in generischer Rock- oder Pop-Musik sehr häufig der Fall ist, sollte die Anzahl der zu analysierenden Takte auf 1 gesetzt werden. Im folgenden Beispiel werden dennoch beispielhaft 4 Takte analysiert (siehe block_size und jump_size).

Den Ablauf der Funktion get_intensity_average_for_next_segment() veranschaulicht das folgende Flussdiagramm.

![](https://lh3.googleusercontent.com/JbWmCgk8NpvX7OyKFsXINAMIGxwkCtrBKpH8vLaPe7oAQWv9--ToywDTsVJuZnPJ53UshSptz0-b "get_intensity_average_for_next_segment")

Neben der beschriebenen Funktionsweise beinhaltet die Funktion noch eine Frequenzweiche, die es zukünftig ermöglichen könnte, bestimmte Frequenzen je nach manuell gewähltem Genre eines Songs stärker oder weniger stark zu gewichten.

### get_intensity_changes()

Um die Generierung der Lightshows zu vereinfachen, wurde eine weitere Funktion geschrieben, die nur noch die Zeit/Intensitäts-Paare ausgibt, die sich um mehr als einen bestimmten Wert (threshold) vom Vorgänger unterscheiden.

So können mehrere Lightshow-"Stufen" realisiert werden, beispielsweise so:

0-15% Intensität: Nur Ambient Lichter
16-50% Intensität: Frequenzabhängige Lichter
51-100% Intensität: Action Lichter, Stroboskop o.Ä.

Den Ablauf der Funktion kann man dem folgenden Flussdiagramm entnehmen.

![enter image description here](https://lh3.googleusercontent.com/7RLl8fBenre-GjchBd-O-jL8HtD0NPpp-gQtetebR2bmOyr2KuLPFtj-ypGy4iJDd2vGl8njs0SN "get_intensity_changes")

## 3.4.5 Hilfsfunktionen

### Gleitender Mittelwertbilder

Unser gleitender Mittelwertbilder berechnet an jedem Messpunkt die durchschnittliche Energie des aktuellen Zeitpunkt, sowie der Messpunkte n-Einheiten nach und n-Einheiten-1 vor dem aktuellen Messpunkt. Dazu benutzen wir einen Vektor mit den Energien aller Zeitpunkte, der in der Funktion get_intensity_function_values() mit folgendem Code erzeugt wurde:
 `(fft_result[i][0]*fft_result[i][0])+ (fft_result[i][1]*fft_result[i][1])`

Unser Fenster ist variabel einstellbar, sollte aber nicht zu groß eingestellt werden, da sonst Änderungen zu spät oder zu früh auftreten können. Ein weiteres Problem ist die Endlichkeit unseres Signals, was zur Folge hat, dass der gleitende Mittelwertbilder erst bei (windowSize/2)-1 beginnen kann, da sonst Speicherfehler auftreten, die dadurch zustande kommen, dass wir bei einem Start vor windowSize/2 – 1  auf Zeitpunkte vor dem Start der Datei zurückgreifen würden, die aber nicht vorhanden sind.

Um Fehler wie den Leck-Effekt zu vermeiden, muss unser Block noch entsprechend gewichtet werden. Wenn auf eine Gewichtung verzichtet wird, multiplizieren wir das Signal mit einer Rechteckfunktion, was eine Verstärkung oder Dämpfung unseres Signals zur Folge hätte [Wikipedia: Gleitender_Mittelwert](https://de.wikipedia.org/wiki/Gleitender_Mittelwert). Unser Fenster wird binomial gewichtet. Dafür benutzen wir die Binomialkoeffizienten aus [Wikipedia: Binomialkoeffizient](https://de.wikipedia.org/wiki/Binomialkoeffizient):
				     ![Test	     ](https://lh3.googleusercontent.com/lQNCrmwKEi28u8Z2gSV60o4sM_SbYbGiPJj1ZBcOTkf1pBUkTWElzqFLZ1YUJfVqk6X1rkhPb6yt "NK")
				     
Um die in der Funktion vorkommenden Fakultäten zu berechnen benutzen wir die rekursive Funktion `factorial`.
![enter image description here](https://lh3.googleusercontent.com/eiJzbZombj-R9520oVUy2O37LHZzG8rhO6-laR4QhUHRHq6cTcP7NTaSb_3VZafn7tb-soEVrdEQ)
Die Funktion nimmt einen vorher übergebenen Wert und multipliziert ihn mit der nächst kleineren ganzen Zahl. Dabei ruft sich die Funktion selbst wieder auf, übergibt dieses mal aber den um 1 verminderten Wert. Die Funktion ruft sich solange selbst auf, bis der letzte übergebene Wert die Zahl 1 enthält. Danach werden rückwärts alle Zahlen miteinander multipliziert. Ein Abbrechen bei 0 oder kleineren Zahlen hätte zu Folge, dass das Ergebnis immer 0 wäre. Die Funktion `factorial` Darf außerdem keine Zahlen, die kleiner als 0 sind, als Startwert enthalten, da dadurch eine unendliche Funktion erzeugt wird.

Alternativ zur rekursiven Funktion kann man auch das Pascalsche Dreieck verwenden, was allerdings in der Implementierung schwieriger zu realisieren war [Wikipedia: Pascalsches Dreieck](https://de.wikipedia.org/wiki/Pascalsches_Dreieck).
![enter image description here](https://lh3.googleusercontent.com/ZikU12x4p8dSO5WKMWA8mlvWVXnHDcXj8dWT_7JMCr5UiF0rPNjDLGQm-cI6syxqv-1uvJYMFna9)
Die Ergebnisse der einzelnen Funktionen sind: 
![enter image description here](https://lh3.googleusercontent.com/nC4Cz_CKpkr9m7WTN-qspHgSz8uWe9TfiLgiu080t4VIZqJDfSGUxXe0dIP3NBNf--02EEkN3fBg)
Der Wert eines "Knoten" des Pascalschen Dreieck ergibt sich aus der Summe der Werte der Knoten links und rechts über diesem aktuellen Knoten. So ergibt sich zum Beispiel der zweiten Knoten der dritten Zeile aus der Summe der Knoten 1 und 2 der zweiten Zeile (1+1 = 2).
Die ersten und letzten Knoten jeder Zeile sind immer 1. Ist kein Knoten vorhanden, gilt der Wert dort als 0.
Durch den gleitenden Mittelwertbilder erkennt man, wann sich ein Lied signifikant ändert, sodass man auf einen anderen "Liedabschnitt“ schließen kann. Ein GMB kann also gut für die Song Segmentation verwendet werden. Zusammen mit der Beat Detection könnte man dann den Zeitwert der Änderung auf den nächstgelegenen Beat legen, um eine Farbänderung der Lampen synchron zu den Taktschlägen zu schalten. Probleme entstehen allerdings, wenn der Mittelwertbilder eine höhere Verzögerung aufweist. Dadurch werden die Zeitwerte den falschen Beats zugeordnet.

Anschließend an jeden Block wird noch die Differenz zu dem Mittelwert der gesamten Datei berechnet, was sich auch für die Song Segmentation hätte eignen können. Diese wurden ebenfalls in einem Vektor gespeichert.


### Hoch-, Tief-, Wendestellen

Die Liederabschnitte werden festgestellt, indem wir die Werte unseres gleitenden Mittelwertbilder auf Hoch- und Tiefpunkte analysieren und dann zwischen jedem Hoch- und Tiefpunkt die Wendestellen ausmachen. An diesen Wendestellen haben wir angenommen, dass es sich dabei um einen neuen Songabschnitt handelt. Hoch- und Tiefpunkte werden erkannt, indem man die Werte des Vektors mit seinem Vorgänger und seinem Nachfolger vergleicht. Dabei gilt:
`if(data(i-1)>data(i)<data(i+1))` --> Tiefpunkt
`if(data(i-1)<data(i)>data(i+1))` --> Hochpunkt

Danach konnten in den Bereichen zwischen Hoch- und Tiefpunkt jeweils die Steigung gemessen werden, um die Wendestellen herauszufinden. Dazu wird die Differenz eines Messpunktes zu seinem Vorgänger berechnet, gespeichert und nur dann überschrieben, wenn die Differenz von zwei anderen Messpunkten höher ist als der aktuell eingespeicherte Wert. Am Ende eines Blockes wird dann der Zeitpunkt mit der maximalen Differenz zurück gegeben.

Das Problem der Funktion ist die Verzögerung und Ungenauigkeit dieser, weshalb sie für die Lightshow nicht verwendet wird.